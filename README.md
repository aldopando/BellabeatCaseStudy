# BellabeatCaseStudy
Bellabeat Case Study 

# Phases of data analysis process about Bellabeat Ivy 


## Ask 


**What is the problem you are trying to solve?**  

Identify trends and patterns from overall smart devices usage data in order to draw insights and how they may apply in a Bellabeat product**  	
 
**How can your insights drive business decisions?**

My insights can influence in how to address the new Bellabeat marketing strategy.

  

**Business Task**

*Draw meaningful insights from smart device usage trends and how they can impact on Bellabeat marketing strategy*


## Prepare

**About the data**
These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.
Prepare Guiding questions
 
 - Where is your data stored?   
 **Data is stored in a collection of datasets on Kaggle**
 **This collection contains several tables across different metrics of personal tracker data**
 **This collection is broken down into two folders,  both with same metrics but each folder contains data in different periods of time**

 - How is the data organized? Is it in long
   or wide format?  
   **The data is organized in two folders, each folder representing a different period of time:**
   
	 - First Folder (11 tables) **2016/03/11 - 2016/04/11** 
		 - `dailyActivity_merged.csv` 
			 - The dataset is in **wide format**.

				-   Each data subject `id` has a row of a combination of multiple attributes such as `TotalSteps, TotalDistance, TrackerDistance, etc`.
				    
				-   Variables are spread across the columns. Columns represent different metrics like steps, distance, active minutes, etc.
				- Each variable has its own column.

		 - `heartrate_seconds_merged.csv`  
			 - The dataset is in **long format**.

				-   Each row is a single observation of the variable `heart rate value`   by a specific user `id` at a specific time `Time`.
				    
				-  The subject user `id` is repeated across multiple rows to represent the value `heart rate`  at different points of time `Time`
				
		- `hourlyCalories_merged.csv`
			- The dataset is in **long format**.
				-  Each row is a single observation of the variable `number of calories burnt`   by a specific user `id` at a specific time `ActivityHour`.
				    
				-  The subject user `id` is repeated across multiple rows to represent a value of `Calories`  at different hours `ActivityHour`.
		
		- `hourlyIntensities_merged.csv`
			- The dataset is in **long format**. It has more than column apparently representing more than one variable. In reality, the second column represents an aggregate function to calculate the **average** of intensity state exhibited during a specific hour by the user. In other words, the column `AverageIntensity` is the statistical summary of the variable `TotalIntensity`.  Therefore, this dataset only contains one variable where each row represents one observation of this variable at a specific time.

		- `hourlySteps_merged.csv` - The dataset is in **long format**.
  
		- `minuteCaloriesNarrow_merged.csv` - The dataset is in **long format**.
     
		- `minuteIntensitiesNarrow_merged.csv` - The dataset is in **long format**.
     			
		- `minuteMETsNarrow_merged.csv` - The dataset is in **long format**.
			
		- `minuteSleep_merged.csv`
			- The dataset is in **wide format**.  Each row represents a unique combination of identifiers (user **id**, **date** and **logId**).
			
		- `minuteStepsNarrow_merged.csv` - The dataset is in **long format**
		
		- `weightLogInfo_merged.csv`
			- The dataset is in **wide format**. Each data subject `id` has a row of a combination of multiple attributes such as `WeightKg, WeightPounds, Fat, BMI, IsManualReport, LogId`

	- Second Folder  (18 tables) **2016/04/11 - 2016/05/11** 
		 - `dailyActivity_merged.csv*` - The dataset is in **wide format**.
			
		 - `dailyCalories_merged.csv` - The dataset is in **long format**.
		
		- `dailyIntensities_merged.csv`
			- The dataset is in **wide format**. In this dataset there is a combination of multiple columns representing attributes for each row. Each column has its own variable.  A single row has atributes such as `SedentaryMinutes, LightlyActiveMinutes, VeryActiveMinutes, ModeratelyActiveDistance, etc.`
		
		- `dailySteps_merged.csv` - The dataset is in **long format**.
			
		- `heartrate_seconds_merged.csv` - The dataset is in **long format**.
			
		- `hourlyCalories_merged.csv` - The dataset is in **long format**.
			
		- `hourlyIntensities_merged.csv` - The dataset is in **long format**.
			
		- `hourlySteps_merged.csv` - The dataset is in **long format**.
			
		- `minuteCaloriesNarrow_merged.csv` - The dataset is in **long format**.
			
		- `minuteCaloriesWide_merged.csv`
			- The dataset is in **wide format**.  *It has multiple columns that contain the same type of data for different time points.*  For example, columns for `"Calories00", "Calories01", "Calories02"... "Calories59"`  
			E.g.
			***Calories05 = calories burned in fifth minute of the hour.***
			
		- `minuteIntensitiesNarrow_merged.csv`
			- The dataset is in **long format**. The same data as "minuteCaloriesWide_merged.csv" but in long format. **The time points (minutes) are spread out through the rows.**
			
		- `minuteIntensitiesWide_merged.csv` - The dataset is in **wide format**.
			
		- `minuteMETsNarrow_merged.csv` - The dataset is in **long format**.
			
		- `minuteSleep_merged.csv` - The dataset is in **wide format**.
			
		- `minuteStepsNarrow_merged.csv` - The dataset is in **long format**.
			
		- `minuteStepsWide_merged.csv` - The dataset is in **wide format**.
			
		- `sleepDay_merged.csv`
			- The dataset is in **wide format**. The number of rows in this dataset can initially suggest a "long" format, especially when it can be observed many repeated IDs or timestamps. But **the format (wide vs. long) is not defined by the number of rows**. it’s defined by **how the variables are organized**
Each row of this dataset is a summary of many variables (`TotalSleepRecords, TotalMinutesAsleep, TotalTimeInBed `) for a specific time `SleepDay`

			
		- `weightLogInfo_merged.csv` - The dataset is in **wide format**.

****
 - Are there issues with bias or credibility in this data? Does your
   data ROCCC?
   
    **Reliability**
	   
	 - ***Uncertainty about bias in the data***: The authors of this dataset doesn't share any information about how participants were chosen for the sample. Therefore, we cannot make sure if the data was collected using random sampling to fairly have a good representation of the population. 
	 - **Small sample**:  This dataset has a sample size of 30 users. Even though it has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population, this dataset is at borderline of this minimum number. A larger sample size would have been better to get more accurate and reliable results from analysis,specially in very large market. According to [statista](https://www.statista.com/forecasts/1425172/fitness-tracker-users-by-segment-us), In 2024, there were around 62 million fitness tracker users in the United States (35% women). In the near future, the number of fitness tracker users is forecast to steadily increase, jumping to over 92 million by 2029. 



    **Original**
   
    The dataset is ***third-party data*** , shared by the user *Möbius* on Kaggle. 
    Möbius has attached the link of the original source which it comes from an **open data source**, under a **Creative Commons Attribution 4.0 International license**. meaning anyone can copy, distribute, remix, and build upon the material for any purpose, even commercially, as long as you give credit for the original creators. 
     In other words, you can have accessibility to the **original data source** under a permissive license to give credit to the **original authors** who collect and generate this dataset.
  
    **Comprehensive**
  
    ***Critical Information needed to solve the business task***
    The Bellabeat app provides users with health data related to their activity, sleep, stress, menstrual cycle, and mindfulness habits. This data can help users better understand their current habits and make healthy decisions. The Bellabeat app connects to their line of smart wellness products
    
    ***Data contained in FitBit Fitness Tracker  datasets***
    FitBit Fitness Tracker Data contains user's information about their heart rate, sleep monitoring, daily activity by tracking totals for steps, intensity, distance, calories, and logged activities.
    We can make sure that the datasets utilized for this analysis cover most of the important metrics that Bellabeat uses to provide insights about its user's health and wellness.
    We are lacking of specific information about reproductive health data for women, but this is an exclusive feature that Bellabeat offers compared to other brands. 
  
    ***Completeness***
    
     - **Small sample size**: The sample size fall short, using only *thirty people* as sample size of a huge population that every year is growing more and more. 
     - **Short period of time**: The data collection was released during two months 2016/03/12 - 2016/05/12. This period of time might be a limitation to track noticeable changes in user's routine. According to [James Clear](https://jamesclear.com/new-habit#:~:text=On%20average%2C%20it%20takes%20more,to%20form%20a%20new%20habit.), "***On average, it takes more than 2 months before a new behavior becomes automatic — 66 days to be exact.** And how long it takes a new habit to form can vary widely depending on the behavior, the person, and the circumstances. In Lally’s study, it took anywhere from 18 days to 254 days for people to form a new habit.*"
  
  
  
    **Current**
   
    Checking the dataset *FitBit Fitness Tracker Data* in its original source, we notice that the last time updated was May 31, 2016. This data is out of date, which it means that the data is less relevant to the current time this analysis is being performed.
    
    **Cited**
  
    These datasets were created by *Robert, Brinton Furberg, Julia Brinton, Michael Keating and Alexa Ortiz*, published and hosted via Zenodo website, a reliable open repository. 
    The data was collected by a distributed survey via Amazon Mechanical Turk between 03/12/2016-05/12/2016. 
  
     *Citation*
    Furberg, R., Brinton, J., Keating, M., & Ortiz, A. (2016). Crowd-sourced Fitbit datasets 03.12.2016-05.12.2016 [Data set]. Zenodo.
    [https://doi.org/10.5281/zenodo.53894](https://doi.org/10.5281/zenodo.53894)
****
 - How are you addressing licensing, privacy, security, and
   accessibility?
  
 ### Authors
 Furberg, Robert.
 Brinton, Julia.
 Keating, Michael.
 Ortiz, Alexa.
 
 ### Citation
   
   Furberg, R., Brinton, J., Keating, M., & Ortiz, A. (2016). Crowd-sourced Fitbit datasets 03.12.2016-05.12.2016 [Data set]. Zenodo.
  
  ### Rights

**License**

 ![cc-by-4.0 icon](https://zenodo.org/static/icons/licenses/cc-by-icon.svg)  
[Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode)
   
   ****
  
 - How does it help you answer your question?     
 
*FitBit Fitness Tracker Data* contain all critical information (such as heart rate, sleep monitoring, and daily activity) to draw insights, identify patterns and trends into how people how people are using wearable devices. It is cited as well and comes from an original and reliable source. The dataset is structured and organized to start cleaning the data and performing analysis.


****
 -  Are there any problems with the data?
 
**Problems and limitations with the data**:

-   **Small sample size:**  This dataset has a sample size of 30 users. The market of people using wearable devices is huge (80 million just in USA) and growing each year. This can be a limitation to get an accurate result of the sample that fairly represents the entire population.  
-   **Out of date**: The data was generated since 2016, making it more irrelevant due to  the analysis is being conducted in 2025.
-   **Limited period of time:** This period of time might be a limitation to track any change in user's routine, which is essential to identify outliers and have a more complete representation of patterns and tends people have in their wellness and daily activity.
-   **Possible biased sample:** The sample may not be a fair representation of the population. We don't have accessibility to details about how participants were eligible to the survey. 
-   **Lack of demographic data**: Age and gender are not specified, this is essential information because Bellabeat products are targeted to women. 
- **Lack of reproductive health data**: Bellabeat tracks reproductive data such as menstrual cycle, pregnancy and fertility. Nevertheless, this dataset doesn't contain any information about these type of metrics.


 **Possible missing data or inconsistencies: Must be checked during processing.**

Key tasks 
1. Download data and store it appropriately. 
2. Identify how it’s organized. 
3. Sort and filter the data. 
4. Determine the credibility of the data. 

**Deliverable.**
 A description of all data sources used


## Process 

Process Guiding questions 
- What tools are you choosing and why? 
**Cleaning data**.
I decided to process and clean the data using SQL because SQL 
- Have you ensured your data’s integrity? 

**Changelog**
Author: Aldair Pichon Aguila.
 
1. I downloaded the datasets from Kaggle in my local machine. 05/20/2025
2. I renamed the two main folders that contain the datasets to ensure better clarity, consistency and organization of the data following guidelines of naming conventions:
	Fitabase Data 3.12.16-4.11.16  ->   FitabaseData_20160312-20160411
	Fitabase Data 4.12.16-5.12.16  ->  FitabaseData_20160412-20160512
  05/20/2025
  
3. I created a new project in BigQuery named `analysisbellabeat246` and two datasets for each of the two folders. 05/20/2025
4. When uploading the table `heartrate_seconds_merged.csv` and checking the auto-detect box to automatically set the schema for this table in BigQuery, then it tries to parse the column `"Time"` as TIMESTAMP data type. Due to the original format of `"Time"`  is one that BigQuery cannot recognized as TIMESTAMP, it returns an error.  To fix this problem, rather than BigQuery auto detecting the schema of the table, I manually specified the schema in JSON format and defining the column `"Time"` as STRING instead of TIMESTAMP.
In order to figure out the name, data type and description of each column I referred to the document [Fitbit Data Dictionary](https://www.fitabase.com/media/2088/fitabase-fitbit-data-dictionary-as-of-4524.pdf). 05/20/2025
 
 
	    [  
			{  
				"description": "Id",  
				"mode": "NULLABLE",  
				"name": "Id",  
				"type": "INTEGER"  
			},  
			{  
				"description": "Time",  
				"mode": "NULLABLE",  
				"name": "Time",  
				"type": "STRING"  
			},  
			{  
				"description": "Value",  
				"mode": "NULLABLE",  
				"name": "Value",  
				"type": "INTEGER"  
			}  
		]

5. I uploaded the rest of tables through manually writing the schema in JSON format for the datasets `FitabaseData_20160312-20160411` and `FitabaseData_20160412-20160512`.
    [Here](SchemaTables.md) you can find the code in JSON format to define the schemma of tables uploaded in BigQuery. Except for the tables `dailyCalories`,  `dailyIntensities`, and `dailySteps`. Their schema created automatically by BigQuery due to it didn't have problems to parse them.
    05/21/2025
 
6. When uploading the tables into the dataset `FitabaseData_20160412-20160512`, I decided to add `_secondPeriod` at the end of each table's name to distinguish these tables from the tables of the `FitabaseData_20160312-20160411` dataset . It will avoid any confusion in the future when merging the tables. This is important because after downloading the datasets from Kaggle I noticed that some table's name repeat across the two folders representing the same information but in different periods of time. 05/21/2025
   
    E.g.
    
    	 `FitabaseData_20160312-20160411 -> heartrate_seconds_merged.csv`
    	 `FitabaseData_20160412-20160512 -> heartrate_seconds_merged.csv`
    	 
    Renamed tables after uploading in BigQuery 
    
    	 `FitabaseData_20160312-20160411 -> heartrate_seconds.csv`
    	 `FitabaseData_20160412-20160512 -> heartrate_seconds_secondPeriod.csv`


7.  Checking for NULL values in each table. In order to figure out how to address possible NULL values in the data, first I need to know if they exist at all and in what columns. I counted the null values for each column in each table of the project. [Here](checkingNulls.md) you can see the queries used and the results in each table. We discovered that only the table "weightLogInfo" in both datasets contain NULL values in the "Fat" column, which is the field for recording the body fat percentage. In the first dataset, 31 out of 33 rows contain NULL values and in the second one, 65 out of 67 rows. Even though some participants data was measured and synched automatically using a scale connected to the Fitbit account, it seems that the version of scale that they used wasn't able to measure and display any information about body fat percentage. The two participants whose body fat percentage was inputted, it was recorded manually. Unfortunely, almost all records in this column contain NULL values, so I decided to remove this column for analysis. Body fat percentage could have been very useful to provide valuable insights about overall health and help make informed decisions about fitness and nutrition goals. Nevertheless, we still have information about BMI metrics and weight for analysis.
   05/26/2025

8. Before deciding which tables to use for the analysis, I went over several of them to determine whether they provide accurate and complete information and are consistent with other tables. 

9. Checking for consistency between minute tables in each dataset.  Allegedlly, each minute table contains a specific metric (calories, steps, METs, intensities, and sleep) that was tracked simultaniously at the same time and for the same users (Id) along with the other metrics. Therefore, I decided to compare and see if the tables contain the same users Id and during the same periods of time. For this task, I used SQL along with pivot tables in Google sheets. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/consistency.md#checking-for-consistency-between-calories-intensity-mets-steps-and-sleep-in-minute-tables) you will see the steps and results about the consistency between minute tables. The results shown that `minuteCalories`,`minuteIntensities`, `minuteMETs`, and `minuteSteps` are consistent between them in both datasets. The information of these tables was recorded simultaneously during the same period of time and with the same users. The `minuteSleep` table is not consistent with the rest of the minute tables, because the data of this table was logged only for some users out of the 33 participants. Hence, `minuteSleep` table is going to be analyze individually.
  05/27/2025    

10. After ensuring the consistency within the minute tables, we proceeded to check the consistency within the hourly tables. By doing this, we can streamline the process by selecting just one minute table and one hourly table for the comparison. **I want to figure out if hourly tables were generated by agreggating the data from minutes to hours**. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/consistency.md#checking-for-consistency-in-hour-tables-calories-intensity-and-steps) you can find the process and results done for checking consistency within hourly tables. We discovered that all hourly tables are consistent each other in both datasets.
    05/27/2025

11. Afterward, I compared the minute and hourly tables to verify whether they represent the same data, aggregated using different time measures. You can find the method and results for this comparison [here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/consistency.md#checking-for-consistency-minutes-vs-hourly-tables).We found that the minute and hourly tables within the `FitabaseData_20160312_20160411` dataset are consistent, meaning they represent the same data. Although we observed differences in the total values between the `minuteCalories` and `hourlyCalories` tables for each user, this inconsistency is due to the data types: `minuteCalories` contains float values (with decimals), while `hourlyCalories` contains integer values. This is because the `minuteCalories` table was created by dividing the calories per hour values by 60, to represent the number of calories burned per minute, which results in decimal values. **Now that we know that minute and hourly tables tables are consistent, I decided to only use the hourly tables for analysis**
In the `FitabaseData_20160412_20160512` dataset, we found inconsistencies between the hourly and minute tables because some users stopped tracking their data at different times in each table. This led to inconsistent total values between the minute and hourly tables.This is unusual, as the hourlyCalories table is supposedly built based on the minuteCalories data. **To address this inconsistency, we decided to not use the hourly tables from the second dataset, instead we will stick with the minute tables and based on them building the hourly tables by ourselves**.
    05/28/2025

12. The `dailyActivity` table in each dataset contains daily totals for steps, intensity, distance, and calories. I assume that the minute and hourly tables are subsets of the `dailyActivity table`. If this table represents the same data as the minute and hourly tables, I might use it for analysis to draw insights about users’ daily activity and health, instead of grouping the data into days manually using the minute and hourly tables. After checkig wheter the `dailyActivity` table is consistent with minute and hour tables, we discovered it is not. Although the `dailyActivity` table covers a time period very similar to the minute and hourly tables, the differences are significant enough to confirm that they are not fully consistent. You can check the process and results [here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/consistency.md#checking-consistency-in-dailyactivity-tables).
**The `dailyActivity` table is not consistent with the rest of minute and hour tables. Therefore, I decided not to use it for our analysis, instead I will build a daily table using the minutes and hourly data**
  05/29/2025

13. However, `hourlyintensities` tables don't contain the `SedentaryMinutes`, `LightlyActiveMinutes`, `FairlyActiveMinutes`, and `VeryActiveMinutes ` columns  to represent the time spent in one of four intensity categories:

Intensity value for the given minute.

0 = Sedentary

1 = Light

2 = Moderate

3 = Very Active


These colunmns are important for our analysis, therefore, we are gonna use the "minuteIntensities" tables from both datasets to calculate the values of these columns and add them in new "hourlyIntensities" tables. In other words, we a won't use the "hourlyIntensities" tables already present in the datasets. We will use the "minuteIntensities" tables four our analysis  to create the hourly and daily tables by ourselves. 05/29/2025

14. After ensuring the data is consistent across the tables in each dataset and contains no NULL values, the following tables have been selected for analysis:


 ### FitabaseData_20160312_20160411 dataset 

| Table  |
| --- |
| hearrate_seconds |
| hourlyCalories |
| minuteIntensitiesNarrow |
| hourlySteps |
| minuteMETsNarrow |
| minuteSleep |
| weightLogInfo |



### FitabaseData_20160412_20160512 dataset

| Table |
| --- |
| heartrate_seconds_secondPeriod |
| minuteCaloriesNarrow_secondPeriod
| minuteIntensitiesNarrow_secondPeriod |
| minuteMETsNarrow_secondPeriod |
| minuteStepsNarrow_secondPeriod |
| minuteSleep_secondPeriod |
| weightLogInfo_secondPeriod |


15. For this analysis, it is important to draw insights and identify patterns that emerged throughout the entire survey period. Therefore, having a complete view of the data is essential. This means I need to merge the tables from the first dataset with those from the second. To do this, I will use the foreign key (user_id) contained in the tables. However, before merging, I need to verify whether the tables contain the same users across both datasets. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/Merging.md#checking-user-consistency-across-datasets-before-merging-tables) you will find the queries and results used for this cleaning process. 05/30/2025
    
    - weightLogInfo: more than half of the participants are missing weight data in one of the datasets, resulting in incomplete information. Additionally, I didn't observe any significant changes in users' weight during the two-month period. For the users who tracked their weight data, the data was recorded sporadically. Therefore, we decided to include all available weight data from both datasets, regardless of whether some users are missing weight information in one of them. Our goal is to use the weight data as an overall indicator of the participants' health status during the survey.  
    - minuteSleep: there are 3 out of 25 participants that are missing sleep data in one of the two datasets. However, this represents a small portion of the overall available sample (although the sample size itself is smaller than what is statistically recommended to fairly represent a population), we will merge the datasets to include only the users who are present in both datasets, resulting in a final sample of 22 participants.
    - calories, intensities, METs and Steps: there are 3 out of 25 participants who are missing data across these tables in one of the two datasets. Nevertheless, this does not represent a major issue in terms of completeness within the available sample. Therefore, we will merge the datasets to include only the users who are present in both datasets, resulting in a final sample of 32 participants.


16. Before merging the tables from both datasets, I will create the new "hourlyIntensities" tables based on the "minuteIntensitiesNarrow" tables.

**FitabaseData_20160312_20160411 dataset**

Query.

	SELECT
	  Id,
	  TIMESTAMP_TRUNC(activityMinute, HOUR) AS activityHour,
	  SUM(Intensity) AS TotalIntensity,
	  ROUND(AVG(Intensity), 1) AS AverageIntensity,
	  COUNTIF(Intensity = 0) AS SedentaryMinutes,
	  COUNTIF(Intensity = 1) AS LightlyActiveMinutes,
	  COUNTIF(Intensity = 2) AS FairlyActiveMinutes,
	  COUNTIF(Intensity = 3) AS VeryActiveMinutes 
	
	FROM ( 
	  SELECT 
	    Id,
	    PARSE_TIMESTAMP('%m/%d/%Y%I:%M:%S %p', ActivityMinute) AS activityMinute,
	    Intensity
	
	  FROM `analysisbellabeat246.FitabaseData_20160312_20160411.minuteIntensitiesNarrow`
	)
	
	GROUP BY Id, activityHour


I saved the results as a new table called `hourlyIntensities_complete` in this dataset.


**FitabaseData_20160412_20160512 dataset**

Query.

	SELECT
	  Id,
	  TIMESTAMP_TRUNC(activityMinute, HOUR) AS activityHour,
	  SUM(Intensity) AS TotalIntensity,
	  ROUND(AVG(Intensity), 1) AS AverageIntensity,
	  COUNTIF(Intensity = 0) AS SedentaryMinutes,
	  COUNTIF(Intensity = 1) AS LightlyActiveMinutes,
	  COUNTIF(Intensity = 2) AS FairlyActiveMinutes,
	  COUNTIF(Intensity = 3) AS VeryActiveMinutes 
	
	FROM ( 
	  SELECT 
	    Id,
	    PARSE_TIMESTAMP('%m/%d/%Y%I:%M:%S %p', ActivityMinute) AS activityMinute,
	    Intensity
	
	  FROM `analysisbellabeat246.FitabaseData_20160412_20160512.minuteIntensitiesNarrow_secondPeriod` 
	)
	
	GROUP BY Id, activityHour


I saved the results as a new table called `hourlyIntensities_secondPeriod_complete` in this dataset.


15. I merged the next tables from the two datasets to get the new merged tables. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/Merging.md#merging-tables) you will find the file with the queries performed to merge the tables. 05/31/2025
    
    | Table || Result |
    | --- | --- | --- |
    | weightLogInfo | weightLogInfo_secondPeriod | weight_data |
    | minuteSleep | minuteSleep_secondPeriod | minuteSleep_merged |
    | hourlyCalories | minuteCaloriesNarrow_secondPeriod | hourlyCalories_merged |
    | hourlyIntensities_complete | hourlyIntensities_secondPeriod_complete | hourlyIntensities_merged |
    | hourlySteps | minuteStepsNarrow_secondPeriod | hourlySteps_merged |
    | minuteMETsNarrow | minuteMETsNarrow_secondPeriod | minuteMETs_merged |

16. After merging the tables from the two datasets, I checked for extra spaces or characters across the records of each merged table. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/cleaning.md#extra-spaces-in-data) you can find all the queries and steps performed to check for extra spaces in the merged tables. **We didn't find any extra spaces or characters in the analyzed columns**. 06/01/2025

17. Finally, I checked for duplicates in the merged tables to finish the cleaning process of this data. [Here](https://github.com/aldopando/BellabeatCaseStudy/blob/main/cleaning.md#duplicates-in-data) you will find the process and results for cleaning duplicates in the data. Since BigQuery doesn't allow updates or deletions of individual rows in standard tables, we will create a dataset that will storage the cleaned version of the tables that contain only distinct rows. This dataset is named `clean_data`. 06/02/2025
    
	- hourlyCalories_merged: 175 duplicate rows were removed.
  	- hourlyIntensities_merged: 175 duplicate rows were removed.
	- hourlySteps_merged:  175 duplicate rows were removed.
	- minuteMETs_merged: 10,500 duplicate rows were removed.
	- weight_data: 2 duplicate rows were removed.
  	- minuteSleep_merged: 4,300 duplicate rows were removed.


### Clean_data dataset

| Table |
| --- | 
| hourlyCalories_cleaned |
| hourlyIntensities_cleaned |
| hourlySteps_cleaned |
| minuteMETs_cleaned |
| weight_data_cleaned |
| minuteSleep_cleaned |


## Analysis

### Aggregating METs from minutes to hours

For this analysis, we will aggregate the data in the `minuteMETs_cleaned` table from minutes to hours in order to merge it along with the other variables (calories, intensities and steps). We will save the results as a new table called `hourlyMETs_cleaned` in our `clean_data` dataset.

To aggregate the variable METs, we will have to sum all MET minutes a person expends during an hour. 

Important: All MET values exported from Fitabase are multiplied by 10. We will divide by 10 to get accurate MET values.

Example: 10 = 1.0 METs; 38 = 3.8 METs


**What is a MET?**

MET stands for Metabolic Equivalent of Task. It is a unit that measures how much energy an activity consumes compared to being at rest. Low-intensity exercises have a lower MET value while high-intensity physical activities have a high MET score.

**Levels of MET**

We can differentiate 3 categories of physical activity:

Light-intensity activities – under 3 MET;
Moderate–intensity activities – from 3 to 6 MET;
Vigorous–intensity activities – over 6 MET.

**What is a MET minute?**

Met-Minutes are used to determine the amount of energy expended during a workout or activity. A MET minute is the amount of energy expended during a minute while at rest.  You can, however, burn more than one MET minute in a minute depending on the intensity of the activity.

**MET minutes per week**

The amount of MET minutes per week tells you how much energy you have expended while performing various activities throughout the whole week.


**World Health Organization guidelines on physical activity and sedentary behaviour**

Currently, the [World Health Organization](https://www.who.int/publications/i/item/9789240015128) recommends adults to meet a minimum physical activity: 

- Adults should aim for at least 150 minutes of moderate-intensity activity or 75 minutes of vigorous-intensity activity per week for minimal health benefits. 
- For additional health benefits, adults should increase their moderate-intensity physical activity to 300 minutes per week or an equivalent

According to [the Skeptical Cardiologist](https://theskepticalcardiologist.com/2021/01/17/the-compendium-of-physical-activities-mets-for-all/) article:
Moderate-intensive activities are ones that cause you to consume at least three times but no more than six times as much energy per minute as you do at rest. Thus moderate intensity exercises or activities are those which require 3-6 METS like walking at 3-4 MPH.

Vigorous activities such as running at >6 MPH burn > 6 METS.

**how many MET minutes is enough?**

Accordig to [omni calculator](https://www.omnicalculator.com/sports/met-minutes-per-week?utm_source=chatgpt.com) website:

MET-minutes/week is a measure used to quantify intentional physical activity, not basic physiological processes like sleeping or resting. Only activities over 3 MET can be considered when counting active minutes per week.

This means that you need at least 450 MET minutes per week to meet these recommendations. Moreover, if we take into account the second recommendation to achieve extra health benefits, you should achieve at least 900 MET minutes per week.

***Therefore, to calculate the MET-minutes for each user in this case study, we will only count the values equal or greater than 3 METs for a more accurate approach to physical activity.*** 


Query.


	CREATE TABLE clean_data.hourlyMETs_cleaned AS
	
	SELECT 
	
	  Id,
	  TIMESTAMP_TRUNC(activityMinute, HOUR) AS activityHour,
	  SUM(CASE WHEN METs/10 > 3 THEN METs ELSE 0 END)/10 AS MET_minutes
	
	FROM `analysisbellabeat246.clean_data.minuteMETs_cleaned` 
	
	GROUP BY Id, TIMESTAMP_TRUNC(activityMinute, HOUR)

---


### We created a new dataset called `analysis` to save all the results after performing calculations and aggregating the data.


### Merging hourly tables (calories, intensities, steps, and METs) + Sorting the data.
 

Query. 

	SELECT  
	  calories.Id,
	  calories.activityHour,
	  calories.Calories,
	  intensities.TotalIntensity,
	  intensities.SedentaryMinutes,
	  intensities.LightlyActiveMinutes,
	  intensities.FairlyActiveMinutes,
	  intensities.VeryActiveMinutes,
	  METs.MET_minutes,
	  steps.StepTotal
	
	FROM `analysisbellabeat246.clean_data.hourlyCalories_cleaned` AS calories
	
	LEFT JOIN `analysisbellabeat246.clean_data.hourlyIntensities_cleaned` AS intensities 
	  ON calories.Id = intensities.Id 
	  AND calories.activityHour = intensities.activityHour
	
	LEFT JOIN `analysisbellabeat246.clean_data.hourlyMETminutes_cleaned` AS METs 
	  ON calories.Id = METs.Id 
	  AND calories.activityHour = METs.activityHour
	
	LEFT JOIN `analysisbellabeat246.clean_data.hourlySteps_cleaned` AS steps
	  ON calories.Id = steps.Id 
	  AND calories.activityHour = steps.activityHour
	
	ORDER BY Id, activityHour



***We saved the results as a new table called `hourlyActivity` in our `analysis` dataset***


**Verifying number of rows**

| hourlyCalories_cleaned | hourlyIntensities_cleaned | hourlyMETs_cleaned | hourlySteps_cleaned | hourlyActivity  |
| --- | --- | --- | --- | --- |
| 44,580 | 44,580 | 44,580 | 44,580 | 44,580 |




### Aggregating the data in the `hourlyActivity` table from hours to days + Sorting the data.


	SELECT  
		Id,
		DATE(TIMESTAMP_TRUNC(activityHour, DAY)) AS activityDate,
		SUM(Calories) AS calories,
		SUM(TotalIntensity) AS totalIntensity,
		SUM(SedentaryMinutes) AS sedentaryMinutes,
		SUM(LightlyActiveMinutes) AS lightlyActiveMinutes,
		SUM(FairlyActiveMinutes) AS fairlyActiveMinutes,
		SUM(VeryActiveMinutes)AS veryActiveMinutes,
		CAST(SUM(MET_minutes) AS INT64) AS MET_minutes,
		SUM(StepTotal) AS totalSteps
		
		
	FROM `analysisbellabeat246.analysis.hourlyActivity` 
		
	GROUP BY Id, activityDate
	ORDER BY Id, activityDate


***We saved the results as a new table called `dailyActivity` in our `analysis` dataset***


### Weekly MET-minutes 

**Oldest and latest date**


	SELECT  
	  MIN(activityDate) AS oldest_date,
	  MAX(activityDate) AS latest_date
	  
	
	FROM `analysisbellabeat246.analysis.dailyActivity` 


| oldest_date | latest_date |
| --- | --- |
| 2016-03-12 | 2016-05-12 |


**Grouping data by week**


